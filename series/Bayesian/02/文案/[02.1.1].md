# 贝叶斯公式和世界观(1): 贝叶斯公式的推导、记忆、和变形 (加长版)
## p1
Hello各位同学大家好，我是fmajor,
我们首先开始第一个主题，
贝叶斯公式的
推导、记忆和变形。
这个视频我制作了两个版本，精简版和加长版
这两个视频，都是为了像大家介绍 后面几期视频中所需要用到的 概念和公式
如果想要快速进入后面那些相对有趣的章节，那么请观看精简版。
如果你本身对贝叶斯公式不是很熟悉，想要我更加耐心的对其进行讲解，那么请观看时长和内容 都超级加倍的 加长版视频。
你正在观看的为加长版。

还有，如果你是直接空降到此视频的话，请先观看第0期前言视频，传送门在这里。
那么我们开始吧
## p2
我们需要简单回顾一下 我推的贝叶斯-从零开始的贝叶斯推导 那期视频的内容。
我们之前 进行了 贝叶斯公理的引入 和贝叶斯体系中定理的推导，大致的逻辑就是（点）
首先提出了一个需求，尝试构建与人类直觉相符合的描述推理过的数学体系。
我们对这个需求进行了解构(点)，引入了 (点)若干公理，然后经过严谨的数学推导得出了(点)这5条定理。
并且给出了概率在贝叶斯体系下的定义，一个定标过的，表示你对事件的感觉，或者说相信程度，再或者说，知识/认知和信念的，一个数值。

但在最后，有的同学就会问，你这推了50分钟，我贝叶斯公式呢？
是呀，在上个视频中，直到最后，我们都没有显式的给出贝叶斯公式，只是给出了这5条定理。
而接下来我们会看到，贝叶斯体系中的所有公式，都是从这几条定理而来。相比于贝叶斯公式，我们更需要牢记的是这几条定理。
# p3
万物起源，就是乘法公式和加法公式。（点）
对乘法公式，进行简单的移项操作，我们就得到了贝叶斯公式的原始形式。(点)
这里的符号需要简单解释一下，一般我们认为 A 是要做判断的事件，或者陈述
而B，我习惯记作background，是新加入的/刚观测到的,与A有关的事情。
而C，我通常理解为Common sense，常识，或者说你所知道的一切。（点）

这个C是必须存在的，而且与使用贝叶斯公式的实体，例如人工智能推理模块，或者说一个人类，所绑定。
但是我们的推导过程中，因为所有的命题均在given C的语境下，所以我们在书写形式上可以进行简化
(我们可以暂时省略掉|C，然后再需要强调C的时候再把它给写上去)。
但是在逻辑上，要记住given C是永远存在，这是 非常重要 的一件事情，我们在之后的视频中也会反复的强调

下文在书写的时候，除非特殊说明，贝叶斯公式均使用简化形式，这是为了节省一些版面，提高推公式的效率（点）

好,右边是贝叶斯公式的简化形式，暂时省略掉|C。
贝叶斯公式的符号表示形式，不止一种。
其他常见的有，使用H，hypothesis, 假设，来代替A
使用E, Evidence，证据，来代替B
或者使用D, Data，数据/观测结果，来代替B
而我们后面的例子，还会用到其他一些更直观的符号

在接下来的推导中，我们使用H假设和D数据来分别代替A和B，以取得一个更加直观的理解。
当然，这是我自己的习惯，这里的符号具体选什么不重要，只要方便你自身理解就可以。（点）

然后就是对贝叶斯公式各部分进行理解的，一个经典的划分。
首先p(H)，我们称之为先验概率，或者先验分布，取决于这个H事件的类型。
关于事件的类型（点），我们可以简单划分为这几类。
比如二值事件,或者说布尔事件，就是事件只存在真或假两个取值，例如明天是否下雨，台风是否登录，都是二值事件
然后还有离散事件，就是事件的结果为离散值，比如色子正面朝上的数值，你抽一张扑克牌的花色，彩票球在某个位置的开奖结果等。
最后是连续事件，此时的事件H的描述一般为，什么什么的值是多少。比如16岁青少年的身高是x, 那么这个时候p(H)就是关于x的一个归一化连续函数，我们一般称之为概率分布，或者概率密度函数。
总之，p(H)可以称之为先验概率，或者先验分布，取决于H事件的类型，我们之后就先把H当做二值事件，此时这个p(H)就是先验概率。
# p4
然后，关于贝叶斯体系的第一个质疑就来了。
凭什么要存在这样一个主观的先验概率。这样算出来的概率是客观的、符合逻辑的么？p(H)取值应该是多少呢?

要回答这些问题，我们就需要把 刚刚才藏起来的C（点）再给他显式的写出来了
先验概率的完整符号表示应该是这样，p(H|C)，C是使用贝叶斯公式的实体所具有的常识，
除非是批量生产和批量初始化的推理机器，否则，每一个实体，比如人或者推理机器，的C，都不一样。
而且，基于我们第一个视频的定义，贝叶斯体系下的概率，本是就是一个跟C有关的，主观的数值。
所以第一个问题的答案就是(点)，凭C。贝叶斯体系下，概率就是主观的，每个实体自身都有常识C，而在她的常识下，p(H|C)是有定义的一个数值。
当然 这个数值本身的计算 有可能比较复杂，比如要把C 展开成很多子事件，然后 用后面推导出的链式法则等公式 来进行计算。
但总之，他是能够算出来的一个数值。

这样算出来的概率是客观的、科学的么? （点）
当然不客观，但是 符合我们的公理和定理，是符合逻辑的。但你又要问了，既然不客观，那么我们是怎么达成一致呢，科学体系还存在么？这个问题，我们就要放在后面的视频再讨论了。
最后，p(H)的取值应该是多少呢？（点）
他应该是p(H|C)，取决于你的C是什么，每个人，都不太一样。
ok这就是关于先验的讨论（点）
然后我们把C再省略掉，继续看其他的部分。
右边分子p(D|H)，我们一般叫做似然，就是在假设成立的条件下，我们得到数据的概率，而下面的p(D)，是数据发生的概率，也叫做evidence，证据，我自己也习惯叫他归一化因子
似然除以归一化因子，称之为概率更新因子。他就是把先验概率值，更新为后验概率值，的这样一个乘数比。
左边，后验概率，是，我得到数据D之后，我对假设H的概率值，而先验概率是，我得到数据之前，我对假设H的概率值。
贝叶斯公式的意义就是，描述，我得到新数据D之后，我对假设H的概率值是怎样变化的，怎样更新的，这样一件过程。
我们之后会进一步进行讨论。
# p5
然后还有其他一些 用得到的公式，我们放在一起，一块儿推导一下。
还是，万物起源：乘法公式和加法公式。
首先，反复使用乘法公式（点），就可以得到链式法则。就是每次展开，把第一个当做A，后面的当做B，反复展开，就得到这个公式，和他的乘表示形式。（点）
然后是，一般加法规则，这里的加法表示的是逻辑运算符or，逻辑或运算。然后基于布尔代数规则，一步一步展开，反复使用乘法公式和加法公式，最终就得到了这样一个公式。（点）
然后是互斥事件组的定义，就是A_i和A_j事件，不可能同时发生。记作一个delta函数。
然后对于互斥事件，他们的逻辑或运算 得到的事件概率 算起来更简单，就 是 他们本身概率值的一个求和，这个证明，只需要 反复使用一般加法规则，然后带入 互斥事件组的定义 就可以了。

还有完备事件组的定义，事件组A_i 逻辑或 运算起来，就得到一个恒真事件。（点）
最后就是互斥完备事件组的全概率公式，
我们把p(B)展开，and一个恒真事件，然后再用完备事件组把他展开，再用互斥事件的加法公式把求和号给拿出来，最后求和号之内使用乘法公式，就得到了这个全概率公式
需要进行记忆的同学请暂停观看，我们就不更加详细的对这些公式进行进一步讨论了。
# p6
然后，我们还需要介绍一下 概率的其他表示方法
首先是概率，probability，这个我们在前面的视频中已经明确的的定义过了。
他的取值范围是0到1，0代表事件A不可能发生，1代表事件A必然发生
然后我们可以对概率值进行一些数学变换，比如下一个，我们定义事件A的odds ratio为p(A)除以p(Abar),或者p除以1减p
这个odds ratio的翻译有很多种，根据其使用场景可以翻译为发生比，优势比，机会比，比值比，让步比等，同时他也在一些情况下表示赔率的倒数。
这里我们就称之为发生比吧。因为对于二值事件A来说，他就是事件A发生的概率除以它不发生的概率。
然后进一步的，我们对发生比取log，就得到evidence,这个单词也有很多种翻译的方法，直接翻译成证据的话，后面有一些表述说起来会怪怪的，所以我这里就翻译成证据量，来明确的表现出，他是一个数值。
其实本来，我想把他翻译成，信念。因为贝叶斯公式，就是描述，你对一件事情的信念如何发生变化，还有你的信念如何转移，上面两句话话说起来及非常的形象和自然。（点）
而且，对于离散事件离散数据的多元假设检验问题，我们还能画出类似这样的我称之为信念转移法阵的图像。
这里面的数值其实就是证据量，如果把他叫做信念会非常的贴切。
但是跟chatgpt辩论过之后，我还是把他翻译成证据量了，因为这样会少一些误解。（点）

使用证据量的一个好处，是 他与我们人类的感官系统 更加的匹配。
我们有一个韦伯-费希纳定理，他的大概意思是，人类的感觉方面的强度，和刺激强度的对数成正比，例如我们的听觉和视觉。
比如表示声音大小，我们就经常拿分贝作为单位，而分贝就是声压相对于参考值的一种对数表示方式。
而对于视觉，正是因为我们的感光设备对于光线强弱的响应是类对数的，才能让我们获得高动态范围的视力，亮的的地方和暗的地方可以同时看清楚。

（点）我们的证据量，可以直接使用10为底，那么他的单位就是dex。但是我们也可以使用更加常见的单位，分贝。
就是10倍log10的发生比。后面的数值例子中，我们也会一直使用分贝作为单位。

对于这三种等效的概率表示方法，我们举一些数值的例子会更加方便理解（点）
比如，概率50%，这个时候，发生比就是5比5，或者说55开，我们的证据量就是0，因为对于这件事情是否发生 没有偏好
而在概率百分之56的情况下，对应着证据量大约为1分贝，这个时候，我们才稍稍 偏信 一些，说A更有可能发生。
我这么说是因为，根据一些估计，一分贝大概是人耳所能分辨出的最小的声音变化幅度。
那我们也可以大致认为，一分贝的证据量变化，也是我们可以感知到的最小的证据量增量，也是我们开始产生偏信这一种感觉的证据量值。
而发生比2比1的情况下，对应着概率为百分之66.7,这个时候，我们的证据量大约有三分贝，我们已经有比较明显的偏信，更相信A要发生而不是A不发生
那如果把发生比倒过来一下，我们的证据量就会变成-3分贝，我们就比较明显的偏信A不发生。

接下来这三个例子，概率值从百分之70，两次增加百分之10%
但是这两次增加所带来的证据量的增量是不相同的，我们可以看到，概率从百分之80增加到百分之90，相比从百分之70增加到百分之80，是需要更多证据的
特别是对于小概率事件和大概率事件，概率接近于0或者接近于1，这个时候，新数据带来的概率值改变也许非常小, 我们对概率值本身的变化，已经没有清晰的概念了
就比如下面这几个例子，概率从百分之99增加到百分之99.9，尽管每次增加的概率值越来越小，但是增加的证据量却是越来越多
此时证据量的改变量才能更好的告诉我们，新数据给我们带来了多少有效的信息，我们之后的例子会对这个问题进行进一步的说明
# p7
这里是一张表示概率值和发生比、还有证据量之间函数关系的图。
横轴是概率值，左边纵轴是发生比但是切换成了log坐标系，右边则是证据量，使用线性坐标系。
红色实线线是发生比曲线，绿色虚线是证据量曲线，由于我把发生比的纵轴坐标也换成了log scale，所以红线就和绿线重合了
可以看出，概率和发生比、证据量之间是单调、连续的函数，所以说，我们知道了三者中任何一个，都可以计算出其他两者，我们三者知其一就可以了，他们是等效的。
唯一有一点可能产生问题的是，发生比和证据量在概率为0或者1的时候是发散的。不过在真实世界中，我们对事物的概率是不会达到0或者1的，只会无限接近于0或者1，这个时候我们的证据量的绝对值会变得很大，但不会是无限大，总还是有会定义的。
除非我们是在探讨数学上才会达到的理论值，比如演绎推理就代表了某个条件概率等于1。所以概率为0或者1在数学上是可以去取到的。
在我们的之后的各种公式中，p(D),p(D ̅ ),p(H),p(H ̅)会频繁的出现在分母上，对其是否为0进行讨论有数学上的严谨性，但是所有公式都拿来讨论比较费文案/版面/视频时长，也没有必要。
所以接下来的所有公式中，我们默认0< P(A)<1，真正的P(A)=0和P(A)=1在实际情况下是达不到的。
对于条件概率同理，除特别情况外，我们也不对其放在分母上时是否为0做额外讨论了 。
在这个条件下，我们所有的概率，都有与之相对应的发生比和信念值，我们很多的讨论就可以放心的进行了。
# p8
好啦，前面铺垫了那么多，我们终于可以开始 仔细的研究贝叶斯公式的各种形态和变换了
我们这里已经使用了H表示假设，D代表数据。后验概率就是我们拿到新的数据D之后，我对假设H的概率值
这第一个形态我称之为记忆形态，因为我们知道，贝叶斯公式就是乘法公式稍微移一下项得到的。
而上面分子是乘法公式的一部分，分母乘以左边是乘法公式的另外一部分，而他们在形式上有非常好的对称性。
所以UP主从来不去记忆贝叶斯公式，每次都是现场推导，先对p(DH)进行乘法公式展开，得到这个p(H)先验概率，然后使用另外一种展开，把后验概率放左边，归一化因子放下面。
所以都是先写分子，再写分母和左边。书写速度和直接默写贝叶斯公式基本上是一样的，因为是一个非常简单的推导。所以大家只需要记忆乘法公式就可以了。（点）
然后就是第二个形态，我称之为先验更新形态。后验概率等于先验概率乘以概率更新因子。表示了在获得新数据D之后，我的先验概率，如何更新到后验概率。表示了一种更新的过程。（点）
接下来是很多考研的同学比较头痛的全概率展开形态。这个倒是也不难记，直接在记忆形态的基础上把归一化因子展开就行了，这里就需要用到我们之前推导出来的互斥完备事件组的全概率公式，而这个公式本身的推导也是不难的，一行就搞定了。可以参考前面的ppt。（点）
然后接下来，二元展开形态，他其实是一种特殊的全概率展开形态，因为一个命题和他的否命题天然构成了互斥完备事件组，所以这个分母就变成了这样两项。
推荐大家去看3blue1brown的这个科普视频，里面会展示对这种形态的一个非常直观地、可视化的理解方式。（点）
最后，是我们接下来视频的重点。我称之二元事件的先验、灵敏度、特异性展开形态。
可以看到我们在二元展开形态的基础上，通过带入加法公式，使得公式中只包含了三种数值。
他们分别是p(H)先验概率，p(D|H)灵敏度，p(Dbar|hbar), 特异性。（点）
我们来仔细看一下他们的定义。
你可以暂时把假设变为患传染病，而数据变为检测结果为阳性
我们定义p(D|H)为灵敏度,sensitivity, 或者叫真阳性率。
而p(Dbar|Hbar)为特异性,specificity,或者叫真阴性率。
这个时候p(D|Hbar)就是1减去真阴性率，或者叫假阳性率，我们称这种条件，把不患病的人测为阳性，为第一类错误，或者说误诊
最后还有p(Dbar|H),就是1减去真阳性率，或者叫假阴性率，我们称这种条件，把患病的人测为阴性，为第二类错误，或者说漏诊

UP在第一次看到看到这四个值的定义和命名的时候，也是脑袋瓜嗡嗡的，记不住，特别是第一类和第二类错误，真假/阴阳 率，傻傻分不清楚。
但是我们视频之后的内容，会紧紧的围绕着灵敏度和特异性，进行展开和讨论。然后再结合一些非常具体的列子。
相信看到最后，大家应该能够自然而然的记住他们的定义。

对于二元事件，我们只需要知道先验、灵敏度和特异性，就可以计算后验概率，而且我们之后会发现，这种方法有他自己的优点。
# p9
然后，接下来，我们来介绍本视频的第二个重要主题
概率更新的贝叶斯因子描述
# p10
我们之前说过，贝叶斯公式描述了，在拿到新数据之后，你对假设的概率认知，是如何更新的，是如何发生变化的。

对于概率值的更新，其实就是乘以了一个似然比。所以似然比也可以叫做概率更新因子。
我们现在把贝叶斯公式变为这种形式，后验除以先验，等于似然比。所以我们知道，如果似然比大于一，那么后验概率将会大于先验概率，则新数据就支持假设H
似然比的计算中，需要计算p(D)，这个数值可以拆成这么两项，或者是上一页中的先验、灵敏度、特异性形式，都需要带入三个数值进行计算

然后我们还提到，概率值还有另外两种表示形式，一个是发生比，还有他的对数值，证据量。
于是我们就可以问，既然概率更新因子是似然比（点），那么发生比更新因子是什么样子呢？
其实发生比更新因子有另外一个名字，也叫做贝叶斯因子，既然都拿老先生的名字来命名了，想必也是一个 比较重要的数值。

我们把它写出来，所谓更新因子就是后验除以先验，那么我们就写一个后验发生比除以先验发生比，这个时候，我们突然发现，这个烦人的p(D)消失了，我们的发生比更新因子更加简洁。
还记得上一页ppt内容的同学会发现，这分子不就是灵敏度，而分母不就是1减去特异性么？对于特定的D与H，这两个值都是 常数，于是发生比更新因子就是一个 与先验先验概率无关的 常数。（点）
那么如果 连续得到独立的数据D_i，则后验发生比 就可以写成先验发生比乘以 数据各自的贝叶斯因子，的这种连乘的形式，非常的优雅。
而如果想要计算得到多个独立数据的概率值，那么每次计算，你都需要带入前一次的先验值，进行一个 迭代的计算。
相较而言，计算发生比更新的连乘形式 要比 计算概率更新的迭代形式，更加的简洁（点）

特别是，如果你对发生比更新的这个公式取log, 写成证据量更新的形式，则连乘就变成了求和，
这个公式，就代表了随着独立数据的获取，我的证据量是怎么样一点一点的变化的。
哎，来了个数据D1，那么我的证据量就改变了log p(D1|H)除以p(D1|Hbar)，或者，改变了log(D1对H的灵敏度除以1减去D1对H的特异性)这是一个固定的数值
哎，又来了个数据D2，那么我的证据量又加上了一个数值，如果连续得到了三个相同的数据D3， 那么我就直接加上三个log(D3的贝叶斯因子)
计算概率的时候所需要的这种复杂的 迭代运算，在计算证据量的时候就变成了 简单的加加减减，
而且数据固定的话，加减的数值都是一个常数，如果出现多个相同的数据，那么只需要加上 常数乘以重复数据个数 就可以了，不需像计算概率那样 反复迭代运算了，我们会在后面的具体例子中进一步体会到 这种计算上的便利性。

 在带入具体实例之前，UP主还是想带大家 对于这些公式，进行一些理论上的探索

首先，很多情况下，我们比较关心的是 概率或者证据改变的方向，而不是他的具体值。这个问题很简单，只要考察似然比或者贝叶斯因子是否大于1就可以了。
因为发生比或者证据量是随着概率单调变化的，那么，我们就应该相信，似然比和贝叶斯因子作为概率和发生比的更新因子，应该具有这样的关系，
（点）他们应该同时大于1，同时小于1或者同时等于1
要证明这个也不难，（点）因为我可以使用贝叶斯因子来表示似然比，就像这样，似然比等于，这里是贝叶斯因子
也可以用似然比来表示贝叶斯因子（点），就像这样，贝叶斯因子等于，这两个是似然比。
这两组相互表示的公式，对于各自数值大于 等于 小于1 进行讨论，就可以得出这组结论。大家可以自己暂停自己证明一下
之所以要强调他们之间的一致性是因为我们下个章节要使用这些关系。因为有的时候，贝叶斯因子好计算，而有的时候反而是概率更新因子好计算。
后面我们讨论的时候，哪个好用，就用那个。其中一个因子与1的关系确定了，另外一个因子与1的关系也就自动确定了。
# p11
好的，我们现在开始对 不同H与D关系之下的 概率更新因子和贝叶斯更新因子 的取值进行一些探索，看看能不能得到一些有趣的结论。

（点）变换贝叶斯公式，得到这个形式，
左边是假设H在数据D的情况下的概率更新因子 的定义，而右边就是似然比。
但是这个公式非常的对称，我们完全可以把左右换一下，右边其实也可以看做数据D 在假设H的情况下的 概率更新因子，
这个时候p(D)，就是一个先验，关于数据的一个先验，而分子上，是关于数据的后验
这个后验可以理解为，基于新假设H，对于得到数据D的一个预期的后验概率，所以这两个后验概率的更新因子是一样的，等式左右两边  都是更新因子的一个定义。
如果对p(Hbar|Dbar)和p(Dbar|Hbar)，进行讨论，我们会得到类似的结果。那我们不妨把这四个后验概率，或者说条件概率分为两组，然后讨论一下他们的概率更新因子的取值，看一下有什么结果。（点）

我们首先讨论第一种情况，H与D为独立/不相关关系
两个事件相互独立，就意味着一个事件的概率不随着另外一个事件的发生而改变，写成表达式就是p(H|D)=p(H)
使用这个表达式，结合贝叶斯公式，可以推导出这些关系
p(D|H)=p(D), p(Hbar|Dbar)=p(Hbar),然后进一步的p(Hbar|D)=p(Hbar)和p(H|Dbar)=p(H)，啊，感觉像是绕口令似的
（点）好让我们来计算一下我们之前分好组的这些量，他们的概率更新因子和贝叶斯因子都是什么
先看p(H|D),带入上面的关系式，啊，概率更新因子是1，贝叶斯因子其实不用再算了，肯定也是1，不过这个例子中，他也是容易计算的，也等于1
P(D|H)的更新因子不用算了，和p(H|D)是一样的也都是1
然后剩下一组后验概率的更新因子，同理可计算，啊，也都是1。

所以我们就得到结论：这些后验概率的更新因子都是1，那就没有更新呗。
所以p(H│D)和p(H ̅│D ̅ )都不会随着D或D ̅的获取而更新, D对于H就没有什么用。
嗯，这是一个，符合预期但是很平凡的结果，我们就当热热身，啊，因为之后还要讨论其他的关系，使用的流程也是完全一样的
# p12
然后我们开始讨论第二种情况，H与D为逻辑推导关系
推导关系如何定义呢？H推导出D，换句话说就是，只要H，则D
那么p(D|H)就等于1, 或者说，灵敏度100%

然后我们再结合贝叶斯公式，就可以推导出另外一个关系，嗯 推导过程在这里，得到，p(Hbar|Dbar)=1
这两个关系，就 对应到了 前面视频中我们提到的内个 演绎推理。
然后，我们还是计算一下这四个后验概率的更新因子。（点）
p(H│D)的更新因子结果是这样的，我们不知道他的具体数值，但是知道 他肯定是大于1的，因为p(D)小于1
更新因子大于1，就意味着我们 更相信这件事情了，这对应着什么呢？哎，不就是合情推理么？
嗯，合情推理的定义在上一期视频中，他的意思其实就是数据D的出现使得你更加相信H，就是更新因子大于1.
然后我们再看p(D│H)的更新。计算他的更新因子其实已经没有意义了，因为无论p(D)的先验是什么，给定H之后，马上他的概率就会更新为1。
这个时候贝叶斯因子其实是发散的，因为概率为1的发生比是无穷大的。
所以这两个后验概率的更新分别对应了合情推理和演绎推理。（点）
然后对于另外一组后验概率的更新也是一样的结果。
所以，当 H与D为 逻辑推导关系的时候，我们可以做到 两个演绎推理 和两个合情推理，所以 H与D相互之间 可以提供非常丰富的信息的。（点）

我们再简单讨论一下H与D的第三种关系。等价关系，可以记作H与D相互推导，这个时候灵敏度和特异性均为100%
最终我们可以得到，所有四个后验概率的贝叶斯因子均发散，后验概率都会被更新为确信，在等价关系的情况下，我们可以做到四个演绎推理。
H与D之间，就不仅仅是提供信息了，他们彼此等价，压根儿就是一回事儿。这又是一个符合预期的平凡结果。
# p13
我们现在还回到逻辑推导关系上来
在中间这两种情况下，我们的贝叶斯因子出现了发散。是因为这两个 数学定义上为0的东西 出现在了分母上。
之前我们提到过，在真实世界中，大部分的概率和条件概率都应该是大于0小于1的，那么这里出现了0或1的情况，我们就需要讨论一下了。
这是因为，严格意义上的逻辑推导关系，大多只在于 数学定义中。
而我们之前认为的，很多，所谓演绎推理，其实都不是真正的演绎推理。（点）

这里 我就拿我之前视频的例子，来 把我自己 批判一番

我们之前引入公理2的时候，举了几个例子，来展现演绎推理和合情推理。
他们都基于 A可以推导出B 的这个前提
但是，除了在第一个 数学推导的例子中，A是 真的可以推出B，灵敏度100%，其他的两个例子中，灵敏度 都达不到100%。
就比如，马上下雨，就一定可以推导出现在是阴天么？（点）那如果我拿出 太阳雨的例子，阁下又该如何应对呢？
对于理论和实验的这个问题，的确，如果这个理论成立，就应该得到实验数据，
但是 （点）在做实验的过程中，也有可能会出现问题，比如 泡利不相容原理的提出者 沃尔夫冈泡利，
他的出现 就经常伴随着 实验室仪器出故障的现象，这种效应 被幽默的称之位 为第二泡利不相容原理
所以 本来应该做出来的实验结果 却没有做出来，T检测D的 灵敏度达不到100%，不能称他们为推导关系。（点）
所以，大家好，欢迎来到真实世界
# p14
H与D最常见的关系，其实是逻辑相关关系

在这里，为方便大家的理解，我再做一次符号替换，把假设H，替换为生病sick，把数据D替换为这个粗体的加号，代表疾病检测结果阳性
这就是大家最熟悉的一个 疾病检测问题了

本来，我们想要实现的是，把S和阳，当做等价关系。即所有的患者 检测都成阳性，所有的阳性者 都是患者。
但是，我们做不到。S和阳，并不满足等价关系。我们必然会犯第一类和第二类错误，灵敏度和特异性均达不到100%
这个时候，我们无法做到演绎推理，只能做到合情推理，但是，合情推理也可以给我们提供 非常丰富的信息。（点）

我们这里简单的带入一些数值来看一下。
检测对疾病的灵敏度，设为95%，特异性，设为90%，那么，第一类错误，误诊，假阳概率就是10%，第二类错误，漏诊，假阴的概率就是5%。（点）
然后 我们再分别讨论一下 这几个后验概率的更新，此时他们也有了 非常明确的的意义。
p(H|D)的概率更新因子，就是 若测得阳性，则 对生病的认知 如何更新，他在数值上同时等于
p(D|H)的概率更新因子，就是 若已经得病，则 对测得阳性结果的(预期) 认知如何更新
这两个后验分布，对应了四个因子，其中有三个，嗯，我们一眼看不出来 他们和1的关系，只有这个p(H|D)的贝叶斯因子 非常的好算，他就等于 灵敏度除以1-特异性，我们带入数值，他是9.5，大于1
所以，其他这三个因子，不用算了，必然也是大于一的。

既然大于一，那么，如果测得阳性，我对自身生病的 概率认知会提升。
但是概率值提升多少呢？因为我没有给先验概率，所以还没法算
但是我对生病的 证据量的增量，可以马上算出来，就是log 9.5 （点）

同样的讨论，对于p(Hbar|Dbar)和p(Dbar|Hbar) 也是一样的，这里我就不展开说了，有兴趣的同学可以暂停，仔细看一下

所以 对于我们这个例子，在给定的这些参数下，每测一次阳，我患病的证据量 就增加log 9.5，而每测一次阴，我没有患病的证据量 就增加log 18，或者说，我患病的证据量 就减少log 18
所以说，这个检测是有效的。无论是阴 还是阳的结果，都能给我提供有效的信息，让我的证据量产生一些改变。
# p15
刚刚的例子中，D对H是一个有效的检验手段。
为了检测S，我们寻找，或者说设计了的实验D, 使得实验的灵敏度和特异性都接近于1
灵敏度接近于1，我们是想要逼近 这个推导关系，想要实现，患者都为阳性，不漏诊
而特异性接近于1，我们是想要逼近 这一组推导关系，他们两个是等价的，互为逆否命题，想要实现，阳性者都是患者，不误诊
若能够实现，那么 患病者就等价于阳性者，这是我们所有的检测 想要达成的目标
但是，现实是，灵敏度和特异性 都是小于1的，我们必定 会犯第一和第二类错误，造成误诊和漏诊，不过只要错误率 在我们可以接受的范围内就可以了。
那么这么这两个错误率 会多大程度上 影响我们认知的更新呢？（点）
我们把这两个贝叶斯因子都给写出来。一个是测得阳，我对生病的 发生比更新。另外一个是测得阴，我对不生病的 发生比更新。
我们看这两个数值 都只跟灵敏度和特异性有关。
对于正常的、我们选出来的检测手段，两者都应该接近1，于是两个贝叶斯因子都应该大于1或者远大于1。
我们说这个检测方法是有效的。（点）

然后我们再来讨论一下 下面的这些特殊情况
灵敏度+特异性等于2，或者说，两者都等于1，这个就是等价关系。两者就是一回事儿
那么灵敏度+特异性等于1呢？这个时候，我们发现两个贝叶斯因子都为1，此时检测是不提供任何信息的，具体的例子我们后面再说。
第三种就更有意思了，灵敏度+特异性<1，这个时候，两个贝叶斯因子都是小于1的。
你拿到数据之后，它甚至给你提供的是反向的信息，不过数据会对逆假设提供正向的信息。关于这个的具体例子，我们也放在后面再说。
# p16
好啦，关于灵敏度、特异性，概率更新因子和贝叶斯因子的 基于公式的理论探索，我们就到这里把。
接下来，我们会使用很多例子，来帮助大家消化和理解 之前的内容。
我们下一个视频见。